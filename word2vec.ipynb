{
 "metadata": {
  "name": "",
  "signature": "sha256:9b29b5790206b5baaf7383cc8d841d304bc49483feed7ce546d135ee533d6a8d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Creating Word2Vec Models "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "[Note: A lot of the code could be made more efficient, especially for reading and cleaning the eebo files.]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Import necessary packages and read in processing functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import os, string, codecs, csv, re\n",
      "from os import listdir\n",
      "from operator import itemgetter\n",
      "from glob import glob\n",
      "from bs4 import BeautifulSoup\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "import numpy as np \n",
      "import pandas as pd  \n",
      "from nltk import metrics, stem, tokenize\n",
      "# Read data from files \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopwords = stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def find_corpus_files(corpusdirectory):\n",
      "    \"\"\"Returns a list of all filenames of corpus files\"\"\"\n",
      "    return glob(corpusdirectory + \"/*.xml\") \n",
      "def strip_tags(s):\n",
      "    s = re.sub(\"[()]\", '', s)\n",
      "    s = re.sub(\"[()]\", '', s)\n",
      "    s = re.sub(\"'\", '', s)\n",
      "    s = re.sub(' ', '', s)\n",
      "    s = re.sub('amp;', '', s)\n",
      "    return s\n",
      "def read_csv(csv_file):\n",
      "    f = codecs.open(csv_file, 'r', 'latin1')\n",
      "    t = csv.DictReader(f)\n",
      "    tcp_list = []\n",
      "    for row in t: \n",
      "        for col in row:\n",
      "            if col == 'TCP' or 'Author' or 'Date' or 'Title': \n",
      "                tcp_list.append(row)\n",
      "            else: \n",
      "                continue\n",
      "    return(tcp_list)\n",
      "def tcp_filelist(filelist_dictionaries, start_date, end_date):\n",
      "    file_list = [] \n",
      "    for c in filelist_dictionaries:\n",
      "        d = c['Date']\n",
      "        if '?' in d: \n",
      "            date = d.replace('?', '')\n",
      "            if '-' in date: \n",
      "                date = date.split('-')[0]\n",
      "                if 'u' in date: \n",
      "                    continue\n",
      "                    date = int(date)\n",
      "            date = int(date)\n",
      "        elif '-' in d: \n",
      "            date=d.split('-')[0]\n",
      "            if '?' in date: \n",
      "                date = date.replace('?', '')\n",
      "                date = int(date)\n",
      "            date = int(date)\n",
      "        elif 'u' in d: \n",
      "            continue\n",
      "        else: \n",
      "            date = d\n",
      "            date = int(date)\n",
      "        if date > start_date and date < end_date:\n",
      "            file_list.append(c['TCP'])\n",
      "    file_list = list(set(file_list))\n",
      "    return(file_list)\n",
      "\n",
      "def data_text_pairing(list_dicts, texts):\n",
      "    full_data = [] \n",
      "    for c in list_dicts: \n",
      "        for x in texts: \n",
      "            if c['TCP'] == x['Name']:\n",
      "                new_dict = {'Text': x['Text'], 'ID': c['TCP'], 'Author': c['Author'], 'Title': c['Title'], 'Date': c['Date']}\n",
      "                full_data.append(new_dict)\n",
      "    return(full_data)\n",
      "def clean_meta(date): \n",
      "    if '-' in date or '?' in date: \n",
      "        date = date.replace('?', '')\n",
      "        date = date.split('-')[0]\n",
      "    return date "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize_only(text):\n",
      "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
      "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
      "    filtered_tokens = []\n",
      "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
      "    for token in tokens:\n",
      "        if re.search('[a-zA-Z]', token):\n",
      "            filtered_tokens.append(token)\n",
      "    return filtered_tokens\n",
      "def read_eebofiles(corpus_directory, file_list):\n",
      "    filenames = [] \n",
      "    for f in find_corpus_files(corpus_directory): ### Need this function in the script as well \n",
      "        filenames.append(f)\n",
      "    fns = [] \n",
      "    for i in file_list:\n",
      "        f = corpus_directory + '/' + i + '.xml'\n",
      "        for x in filenames: \n",
      "            if f == x:\n",
      "                fns.append(f)\n",
      "            else: \n",
      "                continue \n",
      "    texts = [] \n",
      "    for i in fns: \n",
      "        filename = os.path.basename(i).split(\".x\")[0]\n",
      "        fd = codecs.open(i, 'r', encoding='latin1')\n",
      "        fn = fd.read()\n",
      "        fn = fn.replace('\u017f', 's')\n",
      "        fn = fn.encode('ascii', 'xmlcharrefreplace')  ##### trying to change ignore\n",
      "        fn = fn.decode('utf8')\n",
      "        data = BeautifulSoup(fn, 'xml') # Parse files w/ Beautiful Soup\n",
      "        data.prettify()\n",
      "        graphs = data.find_all(['text', 'TEXT'])\n",
      "        for g in graphs: \n",
      "            t = g.text\n",
      "            t = t.replace('\\n', ' ')\n",
      "            #t = t.replace('s', 's')\n",
      "            t = t.replace('|', '')\n",
      "            t = t.replace('\u00c5\u00bf', 's')\n",
      "            t = t.replace('\u00e2\\x80\\x94', '-') \n",
      "            t = t.replace(' \u00e2\\x80\u00a2', '')\n",
      "            t = t.replace('\u00e3\\x80\\x88\u00e2\\x97\\x8a\u00e3\\x80\\x89', ' ')\n",
      "            t = t.replace('\u017f', 's')\n",
      "            t = t.replace('\u017f', 's')\n",
      "            text_dict = {\"Text\": t, \"Name\": filename}\n",
      "        texts.append(text_dict)\n",
      "    return(texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      " Read in EEBO metadata and texts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "csv_f = 'eebo_texts/TCP.csv'  #### CHANGE TO METADATA FILENAME \n",
      "tcp_list = read_csv(csv_f) \n",
      "corpus_directory = 'eebo_xml' ### CHANGE TO EEBO DIRECTORY   eebo_sample  \n",
      "add_stoplist = ['p', 'ii', '&', 'yet', 'de', 'hugh', '(by', '1.', '5',\n",
      "    '&c', 'm', 'st', 'also', 'bee', 'euen', 'vs', 'al', \"'d\", 'y', 'o', 'tho', 'b', 'c', \n",
      "    'd', 'ac', \"'t\", 's']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "refined_list = []\n",
      "for i in tcp_list:\n",
      "    date = clean_meta(i['Date'])\n",
      "    mydict = {'TCP': i['TCP'], 'Author': i['Author'], 'Date': date, 'Title': i['Title']}\n",
      "    refined_list.append(mydict)\n",
      "\n",
      "seen = set()\n",
      "new_list= []\n",
      "for d in refined_list:\n",
      "\tt = tuple(d.items())\n",
      "\tif t not in seen:\n",
      "\t\tseen.add(t)\n",
      "\t\tnew_list.append(d)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_list = tcp_filelist(new_list, 1659, 1681) ## 1659 - 1681"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "texts = read_eebofiles(corpus_directory, file_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_data= data_text_pairing(new_list, texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Check file dates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "date_list = []\n",
      "for c in full_data: \n",
      "    date_list.append(c['Date'])\n",
      "set(date_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 138,
       "text": [
        "{'1649', '1650', '1651'}"
       ]
      }
     ],
     "prompt_number": 138
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Put text in list of lists in order to be processed by the word2vec algo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text_to_wordlist(text, remove_stopwords=True):\n",
      "    # Function to convert a document to a sequence of words,\n",
      "    # optionally removing stop words.  Returns a list of words.\n",
      "    #  \n",
      "    #  Remove non-letters\n",
      "    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
      "    #\n",
      "    # Convert words to lower case and split them\n",
      "    words = text.lower().split()\n",
      "    #\n",
      "    # Optionally remove stop words (false by default)\n",
      "    if remove_stopwords:\n",
      "        #stops = set(stopwords.words(\"english\"))\n",
      "        words = [w for w in words if not w in add_stoplist]\n",
      "    #\n",
      "    # 5. Return a list of words\n",
      "    return words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Download the punkt tokenizer for sentence splitting\n",
      "import nltk.data\n",
      "\n",
      "# Load the punkt tokenizer\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "# Define a function to split a review into parsed sentences\n",
      "def text_to_sentences(text, tokenizer, remove_stopwords=False ):\n",
      "    # Function to split a review into parsed sentences. Returns a \n",
      "    # list of sentences, where each sentence is a list of words\n",
      "    #\n",
      "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "    raw_sentences = tokenizer.tokenize(text.strip())\n",
      "    #\n",
      "    # 2. Loop over each sentence\n",
      "    sentences = []\n",
      "    for raw_sentence in raw_sentences:\n",
      "        # If a sentence is empty, skip it\n",
      "        if len(raw_sentence) > 0:\n",
      "            # Otherwise, call review_to_wordlist to get a list of words\n",
      "            sentences.append( text_to_wordlist( raw_sentence, \\\n",
      "              remove_stopwords ))\n",
      "    #\n",
      "    # Return the list of sentences (each sentence is a list of words,\n",
      "    # so this returns a list of lists\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "'          a   w'"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = []\n",
      "for t in full_data:\n",
      "    sentences += text_to_sentences(t['Text'], tokenizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Set parameters for the Word2Vec. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_features = 300    # Word vector dimensionality                      \n",
      "min_word_count = 40   # Minimum word count                        \n",
      "num_workers = 4       # Number of threads to run in parallel\n",
      "context = 10          # Context window size                                                                                    \n",
      "downsampling = 1e-3   # Downsample setting for frequent words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Run model and save it. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import word2vec\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model2 = word2vec.Word2Vec(sentences, workers=num_workers,\n",
      "                           size=num_features, min_count = min_word_count,\n",
      "                           window = context, sample = downsampling)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# If you don't plan to train the model any further, calling \n",
      "# init_sims will make the model much more memory-efficient.\n",
      "model2.init_sims(replace=True)\n",
      "\n",
      "# It can be helpful to create a meaningful model name and \n",
      "# save the model for later use. You can load it later using Word2Vec.load()\n",
      "model_name = \"300features_40minwords_10context-3.0\"\n",
      "model2.save(model_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Play around with the model features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model2.most_similar('succession')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 155,
       "text": [
        "[('pastours', 0.5359125137329102),\n",
        " ('usurpation', 0.5102887153625488),\n",
        " ('apostolical', 0.5026254653930664),\n",
        " ('successor', 0.4913163185119629),\n",
        " ('successors', 0.4908880293369293),\n",
        " ('title', 0.48536381125450134),\n",
        " ('claimed', 0.4702222943305969),\n",
        " ('successours', 0.4621862769126892),\n",
        " ('hereditary', 0.4557878077030182),\n",
        " ('apostolicall', 0.45309439301490784)]"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#model2.most_similar('rights')\n",
      "model2.most_similar(positive=['soul'], negative=['mind'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 166,
       "text": [
        "[('redeemer', 0.2752203643321991),\n",
        " ('soule', 0.26963579654693604),\n",
        " ('staff', 0.264157235622406),\n",
        " ('footstool', 0.2635291814804077),\n",
        " ('zeph', 0.2615640461444855),\n",
        " ('bridegroom', 0.25841039419174194),\n",
        " ('armor', 0.2555397152900696),\n",
        " ('trodden', 0.24854087829589844),\n",
        " ('sighing', 0.24321967363357544),\n",
        " ('whoso', 0.2422497272491455)]"
       ]
      }
     ],
     "prompt_number": 166
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model2.doesnt_match('heart mind soul'.split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 169,
       "text": [
        "'mind'"
       ]
      }
     ],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model2.most_similar(positive=['right', 'libertie'], negative=['duty'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 154,
       "text": [
        "[('possession', 0.3942076563835144),\n",
        " ('freedome', 0.36904266476631165),\n",
        " ('liberty', 0.35154029726982117),\n",
        " ('conqueror', 0.350799024105072),\n",
        " ('claime', 0.3363698720932007),\n",
        " ('heir', 0.33378127217292786),\n",
        " ('hand', 0.33028262853622437),\n",
        " ('norman', 0.3268415629863739),\n",
        " ('claim', 0.3234514594078064),\n",
        " ('priviledge', 0.31260573863983154)]"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model2.most_similar(positive=['king', 'woman'], negative=['man'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 157,
       "text": [
        "[('elizabeth', 0.40880584716796875),\n",
        " ('queen', 0.3875415325164795),\n",
        " ('kings', 0.3779619634151459),\n",
        " ('yorke', 0.377203106880188),\n",
        " ('princess', 0.3671080470085144),\n",
        " ('eldest', 0.36425578594207764),\n",
        " ('charles', 0.3639237880706787),\n",
        " ('lancaster', 0.3634168207645416),\n",
        " ('k', 0.35930222272872925),\n",
        " ('isabella', 0.3580009639263153)]"
       ]
      }
     ],
     "prompt_number": 157
    }
   ],
   "metadata": {}
  }
 ]
}